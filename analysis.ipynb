{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from modules.model_selection import *\n",
    "from modules.visualize import *\n",
    "from modules.utils import *\n",
    "from modules.feature_extraction import *\n",
    "from modules.tracking import *\n",
    "\n",
    "from scipy.stats import loguniform, uniform, randint\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# output directories\n",
    "DATA_DIR = Path('./dataset')\n",
    "FIGURE_DIR = Path('./figures')\n",
    "CACHE_DIR = Path('./cache')\n",
    "FIGURE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAGS\n",
    "ANIMATE_ALL = False # set to True to generate video visualizations for each sample\n",
    "EVAL = True # set to True to train and evaluate models\n",
    "REFIT = False # set to True rerun hyperparameter tuning\n",
    "WEIGHTED = False # set to True to weight same-patient samples during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse demographic and UPDRS data for each patient\n",
    "demographic_data = DemographicData(DATA_DIR/'demographics.xlsx')\n",
    "updrs_data = UPDRSdata(DATA_DIR/'motor_scales_all_2021.03.08.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load, clean, and subsegment gait recordings\n",
    "pose_files = sorted(list(DATA_DIR.glob('**/*gait*csv')), key=natsort)\n",
    "poses = []\n",
    "ids = []\n",
    "for f in pose_files:\n",
    "    try:\n",
    "        pose = PoseSeries(f, min_duration=3, max_duration=7, tolerance=5) # parameters for subsegmentation\n",
    "        if pose.seg_cnt == 0:\n",
    "            continue\n",
    "        ids.append(pose.id)\n",
    "        poses.append(pose)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to build pose for {f}: {str(e)}.\")\n",
    "ids = np.unique(ids)\n",
    "print(f\"Built {len(poses)} pose series from {len(ids)} patients.\")\n",
    "\n",
    "if ANIMATE_ALL:\n",
    "    for p in tqdm(poses, desc='Pose Animation'):\n",
    "        p.animate(visualize=False, show_good_range=True, save_fig=True)\n",
    "\n",
    "seg_cnts, all_segments = [], []\n",
    "for p in poses:\n",
    "    seg_cnts.append(p.seg_cnt)\n",
    "    all_segments.extend(p.segments)\n",
    "durations = [(s[1]-s[0])*p.dt for s in all_segments]\n",
    "\n",
    "print(f'Number of total segments: {np.sum(seg_cnts)}')\n",
    "print(\n",
    "    f'Segments per Recording: {np.mean(seg_cnts):.2f} +/- {np.std(seg_cnts):.2f}')\n",
    "print(\n",
    "    f'Segment Durations: {np.mean(durations):.2f} +/- {np.std(durations):.2f} s')\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sns.histplot(seg_cnts, ax=ax1, discrete=True, lw=8)\n",
    "sns.histplot(durations, ax=ax2, lw=8)\n",
    "ax1.set_xlabel('# Segments')\n",
    "ax1.set_title('Good Segments per Gait Recording')\n",
    "ax2.set_xlabel('Duration [s]')\n",
    "ax2.set_title('Segment Durations')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load, clean, and subsegment finger tapping recordings\n",
    "hand_files = sorted(list(DATA_DIR.glob('**/*fingertap*csv')), key=natsort)\n",
    "hands = []\n",
    "ids = []\n",
    "for f in hand_files:\n",
    "    try:\n",
    "        hand = HandSeries(f, min_duration=3, max_duration=7, tolerance=5) # parameters for subsegmentation\n",
    "        if hand.seg_cnt == 0:\n",
    "            continue\n",
    "        ids.append(hand.id)\n",
    "        hands.append(hand)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to build hand for {f}: {str(e)}.\")\n",
    "ids = np.unique(ids)\n",
    "print(f\"Built {len(hands)} hand series from {len(ids)} patients.\")\n",
    "\n",
    "if ANIMATE_ALL:\n",
    "    for h in tqdm(hands, desc='Fingertap Animation'):\n",
    "        h.animate(visualize=False, show_good_range=True, save_fig=True)\n",
    "        break\n",
    "\n",
    "seg_cnts, all_segments = [], []\n",
    "for h in hands:\n",
    "    seg_cnts.append(h.seg_cnt)\n",
    "    all_segments.extend(h.segments)\n",
    "durations = [(s[1]-s[0])*h.dt for s in all_segments]\n",
    "\n",
    "print(f'Number of total segments: {np.sum(seg_cnts)}')\n",
    "print(\n",
    "    f'Segments per Recording: {np.mean(seg_cnts):.2f} +/- {np.std(seg_cnts):.2f}')\n",
    "print(\n",
    "    f'Segment Durations: {np.mean(durations):.2f} +/- {np.std(durations):.2f} s')\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sns.histplot(seg_cnts, ax=ax1, discrete=True, lw=8)\n",
    "sns.histplot(durations, ax=ax2, lw=8)\n",
    "ax1.set_xlabel('# Segments')\n",
    "ax1.set_title('Good Segments per Hand Recording')\n",
    "ax2.set_xlabel('Duration [s]')\n",
    "ax2.set_title('Segment Durations')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate gait gait/body features\n",
    "angle_joints = [\n",
    "    ('nose', 'middle_shoulder', 'left_shoulder', 90, True),\n",
    "    ('left_elbow', 'left_shoulder', 'left_hip', 0, False),\n",
    "    ('right_elbow', 'right_shoulder', 'right_hip', 0, False),\n",
    "]\n",
    "distance_joints = [\n",
    "    ('left_wrist', 'left_shoulder', False),\n",
    "    ('right_wrist', 'right_shoulder', False),\n",
    "    ('left_ankle', 'left_hip', False),\n",
    "    ('right_ankle', 'right_hip', False),\n",
    "]\n",
    "x_displacement_joints = [\n",
    "    ('left_ankle', 'right_ankle', False),\n",
    "    ('left_knee', 'right_knee', False),\n",
    "    ('middle_shoulder', 'middle_hip', True),\n",
    "]\n",
    "y_displacement_joints = [\n",
    "    ('left_ankle', 'right_ankle', False),\n",
    "    ('left_shoulder', 'right_shoulder', False),\n",
    "    ('left_hip', 'right_hip', False),\n",
    "]\n",
    "\n",
    "\n",
    "def min_max_diff(f1, f2, c):\n",
    "    c_max = [cc + '_max' for cc in c]\n",
    "    c_min = [cc + '_min' for cc in c]\n",
    "    f_max = np.maximum(f1, f2)\n",
    "    f_min = np.minimum(f1, f2)\n",
    "\n",
    "    f = f_max.tolist() + f_min.tolist()\n",
    "    c = c_max + c_min\n",
    "\n",
    "    f[::2] = f_max.tolist()\n",
    "    f[1::2] = f_min.tolist()\n",
    "\n",
    "    c[::2] = c_max\n",
    "    c[1::2] = c_min\n",
    "\n",
    "    return f, c\n",
    "\n",
    "\n",
    "all_feats, all_scores = [], []\n",
    "feat_cols, score_cols = None, None\n",
    "\n",
    "# include temp measures to resolve issue with unilateral features\n",
    "for i, p in enumerate(tqdm(poses)):\n",
    "    info = [p.id]\n",
    "    scores = updrs_data.get_all_scores(p.id, p.on_med).to_numpy().ravel()\n",
    "    for j in range(p.seg_cnt):\n",
    "        feats, col_names = [], []\n",
    "\n",
    "        # for joints in angle_joints:\n",
    "        #     base_name = f'{{{joints[0]}, {joints[1]}, {joints[2]}}} (angle)'\n",
    "        #     angle = p.get_angle(*joints[0:4])[j]\n",
    "        #     f, c = get_all_features(angle, base_name, p.dt, joints[-1])\n",
    "        #     feats.extend(f)\n",
    "        #     col_names.extend(c)\n",
    "\n",
    "        for joints in angle_joints[0:1]:\n",
    "            base_name = f'{{{joints[0]}, {joints[1]}, {joints[2]}}} (angle)'\n",
    "            angle = p.get_angle(*joints[0:4])[j]\n",
    "            f, c = get_all_features(angle, base_name, p.dt, joints[-1])\n",
    "            feats.extend(f)\n",
    "            col_names.extend(c)\n",
    "\n",
    "        for j1, j2 in zip(angle_joints[1::2], angle_joints[2::2]):\n",
    "            base_name = f'{{{j1[0].split(\"_\")[-1]}, {j1[1].split(\"_\")[-1]}, {j1[2].split(\"_\")[-1]}}} (angle)'\n",
    "            f1, c = get_all_features(p.get_angle(\n",
    "                *j1[0:4])[j], base_name, p.dt, j1[-1])\n",
    "            f2, _ = get_all_features(p.get_angle(\n",
    "                *j2[0:4])[j], base_name, p.dt, j2[-1])\n",
    "            f, c = min_max_diff(f1, f2, c)\n",
    "            feats.extend(f)\n",
    "            col_names.extend(c)\n",
    "\n",
    "        # for joints in distance_joints:\n",
    "        #     base_name = f'{{{joints[0]}, {joints[1]}, {joints[2]}}} (distance)'\n",
    "        #     distance = p.get_distance(*joints[0:2])[j]\n",
    "        #     f, c = get_all_features(distance, base_name, p.dt, joints[-1])\n",
    "        #     feats.extend(f)\n",
    "        #     col_names.extend(c)\n",
    "\n",
    "        for j1, j2 in zip(distance_joints[::2], distance_joints[1::2]):\n",
    "            base_name = f'{{{j1[0].split(\"_\")[-1]}, {j1[1].split(\"_\")[-1]}}} (distance)'\n",
    "            f1, c = get_all_features(p.get_distance(\n",
    "                *j1[0:2])[j], base_name, p.dt, j1[-1])\n",
    "            f2, _ = get_all_features(p.get_distance(\n",
    "                *j2[0:2])[j], base_name, p.dt, j2[-1])\n",
    "            f, c = min_max_diff(f1, f2, c)\n",
    "            feats.extend(f)\n",
    "            col_names.extend(c)\n",
    "\n",
    "        for joints in x_displacement_joints:\n",
    "            base_name = f'{{{joints[0]}, {joints[1]}}} (x_displacement)'\n",
    "            x_displacement = p.get_x_displacement(*joints[0:2])[j]\n",
    "            f, c = get_all_features(\n",
    "                x_displacement, base_name, p.dt, joints[-1])\n",
    "            feats.extend(f)\n",
    "            col_names.extend(c)\n",
    "\n",
    "        for joints in y_displacement_joints:\n",
    "            base_name = f'{{{joints[0]}, {joints[1]}}} (y_displacement)'\n",
    "            y_displacement = p.get_y_displacement(*joints[0:2])[j]\n",
    "            f, c = get_all_features(\n",
    "                y_displacement, base_name, p.dt, joints[-1])\n",
    "            feats.extend(f)\n",
    "            col_names.extend(c)\n",
    "\n",
    "        all_feats.append(np.concatenate([info, feats]))\n",
    "        all_scores.append(np.concatenate([info, scores]))\n",
    "        feat_cols = np.concatenate([['pidn'], col_names])\n",
    "        score_cols = np.concatenate([['pidn'], UPDRSdata.score_names])\n",
    "\n",
    "feat_df = pd.DataFrame(all_feats, columns=feat_cols).set_index('pidn')\n",
    "score_df = pd.DataFrame(all_scores, columns=score_cols).set_index('pidn')\n",
    "feat_df.to_csv(CACHE_DIR/'gait_features.csv')\n",
    "score_df.to_csv(CACHE_DIR/'gait_scores.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate finger tapping/hand features\n",
    "distance_joints = [\n",
    "    ('right_thumb', 'right_index'),\n",
    "    ('left_thumb', 'left_index'),\n",
    "\n",
    "    ('right_middle', 'right_wrist'),\n",
    "    ('left_middle', 'left_wrist'),\n",
    "\n",
    "    ('right_ring', 'right_wrist'),\n",
    "    ('left_ring', 'left_wrist'),\n",
    "\n",
    "    ('right_pinky', 'right_wrist'),\n",
    "    ('left_pinky', 'left_wrist'),\n",
    "]\n",
    "\n",
    "all_feats, all_scores = [], []\n",
    "feat_cols, score_cols = None, None\n",
    "is_right = []\n",
    "\n",
    "for i, h in enumerate(tqdm(hands)):\n",
    "    info = [h.id]\n",
    "    scores = updrs_data.get_all_scores(h.id, h.on_med).to_numpy().ravel()\n",
    "    for j in range(h.seg_cnt):\n",
    "        feats, col_names = [], []\n",
    "\n",
    "        wrist_r = h.get_joint('right_wrist', raw=True)[j][:, 1]\n",
    "        wrist_l = h.get_joint('left_wrist', raw=True)[j][:, 1]\n",
    "        is_right = wrist_r.mean() < wrist_l.mean()  # note: for y-coordinate, top is 0\n",
    "\n",
    "        for (joints_r, joints_l) in zip(distance_joints[::2], distance_joints[1::2]):\n",
    "            distance_r = h.get_distance(*joints_r)[j]\n",
    "            distance_l = h.get_distance(*joints_l)[j]\n",
    "            distance = distance_r if is_right else distance_l\n",
    "\n",
    "            base_name = f\"{{{joints_r[0].split('_')[-1]}, {joints_r[1].split('_')[-1]}}} (distance)\"\n",
    "            f, c = get_all_features(distance, base_name, h.dt)\n",
    "            feats.extend(f)\n",
    "            col_names.extend(c)\n",
    "\n",
    "        all_feats.append(np.concatenate([info, feats]))\n",
    "        all_scores.append(np.concatenate([info, scores]))\n",
    "        feat_cols = np.concatenate([['pidn'], col_names])\n",
    "        score_cols = np.concatenate([['pidn'], UPDRSdata.score_names])\n",
    "\n",
    "feat_df = pd.DataFrame(all_feats, columns=feat_cols).set_index('pidn')\n",
    "score_df = pd.DataFrame(all_scores, columns=score_cols).set_index('pidn')\n",
    "feat_df.to_csv(CACHE_DIR/'fingertap_features.csv')\n",
    "score_df.to_csv(CACHE_DIR/'fingertap_scores.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate design matrix for gait/body feature set\n",
    "gait_features = pd.read_csv(CACHE_DIR/'gait_features.csv', index_col=0)\n",
    "gait_scores = pd.read_csv(CACHE_DIR/'gait_scores.csv', index_col=0)\n",
    "\n",
    "gait_features['id'] = np.arange(len(gait_features))\n",
    "gait_scores['id'] = np.arange(len(gait_scores))\n",
    "\n",
    "mask = ~gait_scores.isna().any(axis=1)\n",
    "mask &= ~gait_features.isna().any(axis=1)\n",
    "gait_features = gait_features[mask]\n",
    "gait_scores = gait_scores[mask]\n",
    "\n",
    "gait_labels = gait_scores[['3.total', 'id']]\n",
    "low = gait_labels['3.total'] <= 32\n",
    "gait_labels.loc[low, '3.total'] = 0\n",
    "gait_labels.loc[~low, '3.total'] = 1\n",
    "gait_labels = gait_labels.rename(columns={'3.total': 'label'}).astype(int)\n",
    "\n",
    "tmp = gait_features.reset_index().set_index(['pidn'])\n",
    "print(f'Samping from {len(np.unique(tmp.index))} valid patient entries:')\n",
    "print(\n",
    "    f'    Feature Matrix: {gait_features.shape[0]} x {gait_features.shape[1]}')\n",
    "print()\n",
    "print(f'Total Score Labels [Cutoff = 32]:')\n",
    "print(f'    # Mild: {gait_labels.value_counts(subset=\"label\")[0]}')\n",
    "print(f'    # Severe: {gait_labels.value_counts(subset=\"label\")[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate design matrix for finger tapping/hand feature set\n",
    "ft_features = pd.read_csv(CACHE_DIR/'fingertap_features.csv', index_col=0)\n",
    "ft_scores = pd.read_csv(CACHE_DIR/'fingertap_scores.csv', index_col=0)\n",
    "\n",
    "ft_features['id'] = np.arange(len(ft_features))\n",
    "ft_scores['id'] = np.arange(len(ft_scores))\n",
    "\n",
    "mask = ~ft_scores.isna().any(axis=1)\n",
    "mask &= ~ft_features.isna().any(axis=1)\n",
    "ft_features = ft_features[mask]\n",
    "ft_scores = ft_scores[mask]\n",
    "\n",
    "ft_labels = ft_scores[['3.total', 'id']]\n",
    "low = ft_labels['3.total'] <= 32\n",
    "ft_labels.loc[low, '3.total'] = 0\n",
    "ft_labels.loc[~low, '3.total'] = 1\n",
    "ft_labels = ft_labels.rename(columns={'3.total': 'label'}).astype(int)\n",
    "\n",
    "tmp = ft_features.reset_index().set_index(['pidn'])\n",
    "print(f'Samping from {len(np.unique(tmp.index))} valid patient entries:')\n",
    "print(f'    Feature Matrix: {ft_features.shape[0]} x {ft_features.shape[1]}')\n",
    "print()\n",
    "print(f'Total Score Labels [Cutoff = 32]:')\n",
    "print(f'    # Mild: {ft_labels.value_counts(subset=\"label\")[0]}')\n",
    "print(f'    # Severe: {ft_labels.value_counts(subset=\"label\")[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairwise combine body and hand features to generate design matrix for combined/integrated feature set\n",
    "int_features = gait_features.reset_index().merge(\n",
    "    ft_features.reset_index(), on='pidn')\n",
    "int_features = int_features.set_index('pidn')\n",
    "int_features = int_features.rename(columns={'id_x': 'id', 'id_y': 'id2'})\n",
    "\n",
    "int_scores = []\n",
    "for id in int_features['id']:\n",
    "    int_scores.append(gait_scores[gait_scores['id'] == id])\n",
    "int_scores = pd.concat(int_scores)\n",
    "\n",
    "int_labels = int_scores[['3.total', 'id']]\n",
    "low = int_labels['3.total'] <= 32\n",
    "int_labels.loc[low, '3.total'] = 0\n",
    "int_labels.loc[~low, '3.total'] = 1\n",
    "int_labels = int_labels.rename(columns={'3.total': 'label'}).astype(int)\n",
    "\n",
    "tmp = int_features.reset_index().set_index(['pidn'])\n",
    "tmp2 = int_features.drop_duplicates(subset=\"id\")\n",
    "print(f'Samping from {len(np.unique(tmp.index))} valid patient entries:')\n",
    "print(\n",
    "    f'    Feature Matrix: {int_features.shape[0]} x {int_features.shape[1]}')\n",
    "print()\n",
    "\n",
    "tmp = int_labels.drop_duplicates(subset='id')\n",
    "print(f'Total Score Labels [Cutoff = 32]:')\n",
    "print(f'    # Mild: {int_labels.value_counts(subset=\"label\")[0]}')\n",
    "print(f'    # Severe: {int_labels.value_counts(subset=\"label\")[1]}')\n",
    "print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models to evaluate\n",
    "models = {\n",
    "    'SVM': LinearSVC(dual=False, fit_intercept=False, max_iter=5000),\n",
    "    'LR': LogisticRegression(penalty='elasticnet', solver='saga', fit_intercept=False, max_iter=5000),\n",
    "    'LDA': LinearDiscriminantAnalysis(solver='lsqr'),\n",
    "    'RF': RandomForestClassifier(),\n",
    "    'AB': AdaBoostClassifier(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'GNB': GaussianNB(),\n",
    "}\n",
    "# search space for models\n",
    "params = {\n",
    "    'SVM': {'penalty': ['l1', 'l2'],\n",
    "            'C': loguniform(1e-3, 1e0)},\n",
    "    'LR': {'C': loguniform(1e-3, 1e0),\n",
    "           'l1_ratio': uniform(0, 1)},\n",
    "    'LDA': [{'shrinkage': [None, 'auto']},\n",
    "            {'shrinkage': uniform(0, 1)}],\n",
    "    'RF': [{'max_depth': randint(3, 12),\n",
    "            'min_samples_split': randint(2, 10),\n",
    "            'min_samples_leaf': randint(1, 10)},\n",
    "           {'max_depth': [None],\n",
    "            'min_samples_split': randint(2, 10),\n",
    "            'min_samples_leaf': randint(1, 10)}],\n",
    "    'AB': {'learning_rate': loguniform(1e-3, 1e0)},\n",
    "    'KNN': {'n_neighbors': randint(2, 30)},\n",
    "    'GNB': {},\n",
    "}\n",
    "alphas = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not REFIT:\n",
    "    models = {\n",
    "        'SVM': LinearSVC(penalty='l2',\n",
    "                         C=0.01,\n",
    "                         dual=False,\n",
    "                         fit_intercept=False,\n",
    "                         max_iter=5000,\n",
    "                         class_weight='balanced'),\n",
    "        'LR': LogisticRegression(penalty='elasticnet',\n",
    "                                 l1_ratio=0.2,\n",
    "                                 C=1,\n",
    "                                 solver='saga',\n",
    "                                 fit_intercept=False,\n",
    "                                 max_iter=5000,\n",
    "                                 class_weight='balanced'),\n",
    "        'LDA': LinearDiscriminantAnalysis(shrinkage='auto', solver='lsqr'),\n",
    "        'RF': RandomForestClassifier(n_estimators=100,\n",
    "                                     max_depth=8,\n",
    "                                     min_samples_split=2,\n",
    "                                     min_samples_leaf=1,\n",
    "                                     class_weight='balanced'),\n",
    "        'AB': AdaBoostClassifier(learning_rate=0.1),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=9),\n",
    "        'GNB': GaussianNB(),\n",
    "    }\n",
    "    params = defaultdict(dict)\n",
    "    alphas = [0.030774356208980617, 0.024757157946593427, 0.02141453563853955, 0.018523222156741202, 0.008341801332506338,\n",
    "                0.005398628767382501, 0.011149209970080915, 0.02862153445389273, 0.007215521357901021, 0.02862153445389273,\n",
    "                0.009643883791544459, 0.008969245378672715, 0.011987818459583773, 0.011149209970080915, 0.03308910644196496,\n",
    "                0.030774356208980617, 0.02141453563853955, 0.007215521357901021, 0.01490144370527747, 0.007758250168566794,\n",
    "                0.03308910644196496, 0.011987818459583773, 0.008969245378672715]\n",
    "\n",
    "if EVAL:\n",
    "    results = eval_all(\n",
    "        models, params, gait_features, gait_labels, n_repeats=1, alphas=alphas, weighted=WEIGHTED)\n",
    "    with open(CACHE_DIR/'gait.dat', 'wb') as f:\n",
    "        pickle.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not REFIT:\n",
    "    models = {\n",
    "        'SVM': LinearSVC(penalty='l2',\n",
    "                         C=0.001,\n",
    "                         dual=False,\n",
    "                         fit_intercept=False,\n",
    "                         max_iter=5000,\n",
    "                         class_weight='balanced'),\n",
    "        'LR': LogisticRegression(penalty='l2',\n",
    "                                 C=0.0001,\n",
    "                                 dual=False,\n",
    "                                 solver='liblinear',\n",
    "                                 fit_intercept=False,\n",
    "                                 max_iter=5000,\n",
    "                                 class_weight='balanced'),\n",
    "        'LDA': LinearDiscriminantAnalysis(shrinkage='auto', solver='lsqr'),\n",
    "        'RF': RandomForestClassifier(n_estimators=100,\n",
    "                                     max_depth=5,\n",
    "                                     min_samples_split=2,\n",
    "                                     min_samples_leaf=1,\n",
    "                                     class_weight='balanced'),\n",
    "        'AB': AdaBoostClassifier(learning_rate=0.05),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "        'GNB': GaussianNB(),\n",
    "    }\n",
    "    params = defaultdict(dict)\n",
    "    alphas = [0.10558981944335787, 0.04422514763163044, 0.06833538873292302, 0.10558981944335787, 0.02862153445389273, \n",
    "              0.09820327790631257, 0.05112830759482943, 0.084944224982638, 0.09133346228248625, 0.084944224982638, \n",
    "              0.04755162406834014, 0.09820327790631257, 0.026619313461261316, 0.09133346228248625, 0.059108990642279875, \n",
    "              0.03825402746632875, 0.10558981944335787, 0.01602228340877477, 0.04422514763163044, 0.09133346228248625, \n",
    "              0.03308910644196496, 0.084944224982638, 0.07900194712408971, 0.06355498291362295, 0.03557796490339495]\n",
    "\n",
    "if EVAL:\n",
    "    results = eval_all(models, params, ft_features, ft_labels, n_repeats=32, alphas=alphas, weighted=WEIGHTED)\n",
    "    with open(CACHE_DIR/'fingertap.dat', 'wb') as f:\n",
    "        pickle.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not REFIT:\n",
    "    models = {\n",
    "        'SVM': LinearSVC(penalty='l1',\n",
    "                         C=0.005,\n",
    "                         dual=False,\n",
    "                         fit_intercept=False,\n",
    "                         max_iter=5000,\n",
    "                         class_weight='balanced'),\n",
    "        'LR': LogisticRegression(penalty='elasticnet',\n",
    "                                 l1_ratio=0.7,\n",
    "                                 C=0.01,\n",
    "                                 dual=False,\n",
    "                                 solver='saga',\n",
    "                                 fit_intercept=False,\n",
    "                                 max_iter=5000,\n",
    "                                 class_weight='balanced'),\n",
    "        'LDA': LinearDiscriminantAnalysis(shrinkage=0.5, solver='lsqr'),\n",
    "        'RF': RandomForestClassifier(n_estimators=100,\n",
    "                                     max_depth=3,\n",
    "                                     min_samples_split=2,\n",
    "                                     min_samples_leaf=1,\n",
    "                                     class_weight='balanced'),\n",
    "        'AB': AdaBoostClassifier(learning_rate=0.5),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=20),\n",
    "        'GNB': GaussianNB(),\n",
    "    }\n",
    "    params = defaultdict(dict)\n",
    "    alphas = [0.03825402746632875, 0.07347536163492151, 0.0411313750341857, 0.04422514763163044, 0.002810728502080728, \n",
    "              0.07900194712408971, 0.059108990642279875, 0.06833538873292302, 0.059108990642279875, 0.05112830759482943, \n",
    "              0.06355498291362295, 0.06833538873292302, 0.05112830759482943, 0.059108990642279875, 0.07900194712408971, \n",
    "              0.06833538873292302, 0.06355498291362295, 0.0411313750341857, 0.002102977594546134, 0.07900194712408971]\n",
    "\n",
    "if EVAL:\n",
    "    results = eval_all(models, params, int_features, int_labels, n_repeats=32, alphas=alphas, weighted=WEIGHTED)\n",
    "    with open(CACHE_DIR/'integrated.dat', 'wb') as f:\n",
    "        pickle.dump(results, f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize LASSO results\n",
    "def get_count(results):\n",
    "    selected = []\n",
    "    for _, v in results['selected'].items():\n",
    "        selected.extend(v)\n",
    "    counts = [len(s) for s in selected]\n",
    "    return np.array(counts)\n",
    "\n",
    "\n",
    "with open('./cache/gait.dat', 'rb') as f:\n",
    "    a = pickle.load(f)\n",
    "with open('./cache/fingertap.dat', 'rb') as f:\n",
    "    b = pickle.load(f)\n",
    "with open('./cache/integrated.dat', 'rb') as f:\n",
    "    c = pickle.load(f)\n",
    "\n",
    "a, b, c = get_count(a), get_count(b), get_count(c)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 2))\n",
    "sns.histplot(data=a, stat='proportion', ax=plt.gca(), binwidth=5)\n",
    "sns.histplot(data=b, stat='proportion', ax=plt.gca(), binwidth=5)\n",
    "sns.histplot(data=c, stat='proportion', ax=plt.gca(), binwidth=5)\n",
    "\n",
    "plt.savefig(FIGURE_DIR/'lasso.png')\n",
    "\n",
    "print(f'Body: mean={a.mean():.2f}, std={a.std():.2f}')\n",
    "print(f'Hand: mean={b.mean():.2f}, std={b.std():.2f}')\n",
    "print(f'Combined: mean={c.mean():.2f}, std={c.std():.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize gait dataset CV results\n",
    "with open('./cache/gait.dat', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "gait_selected = VisualizeAll(\n",
    "    gait_features, gait_labels, results, out_dir=FIGURE_DIR/'g')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize finger tapping dataset CV results\n",
    "with open('./cache/fingertap.dat', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "ft_selected = VisualizeAll(ft_features, ft_labels,\n",
    "                           results, out_dir=FIGURE_DIR/'ft')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize combined dataset CV results\n",
    "with open('./cache/integrated.dat', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "int_selected = VisualizeAll(\n",
    "    int_features, int_labels, results, out_dir=FIGURE_DIR/'int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP analysis w/ best model\n",
    "selected = int_selected['feat'].to_numpy()\n",
    "\n",
    "clf = LogisticRegression(penalty='elasticnet',\n",
    "                         dual=False,\n",
    "                         solver='saga',\n",
    "                         fit_intercept=False,\n",
    "                         max_iter=5000,\n",
    "                         class_weight='balanced')\n",
    "\n",
    "shap_values, X_test_array = model_shap(clf, int_features, int_labels, selected=selected)\n",
    "shap.summary_plot(shap_values, X_test_array, feature_names=selected, plot_size=(15, 10))\n",
    "shap.summary_plot(shap_values, X_test_array, feature_names=selected, plot_size=(15, 10), plot_type=\"bar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38c80501d2b2e4a9d6df779588d376b0530846b06a79a3e25422ca44224fd643"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
